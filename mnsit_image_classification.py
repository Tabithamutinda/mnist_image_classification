# -*- coding: utf-8 -*-
"""Mnsit_image_classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OEk1mZ59KFe148VJQgSu520Ihd9m34Nr

# Image Classification of Handwritten Digits
This refers to the task of identifying and categorizing images of handwritten numbers.
Handwritten digit recognition has several applications:

- **OCR**: Converts scanned or handwritten documents into digital text, used in document digitization and banking for check processing.
- **Postal Systems**: Reads handwritten postal codes to automate mail sorting.
- **Healthcare**: Digitizes handwritten medical notes for electronic records.
- **Education**: Converts handwritten math and science notes for interactive learning apps.
- **Security and Archiving**: Verifies input and digitizes important records.
- **Mobile Apps**: Enhances note-taking apps by converting handwriting to digital text.
- These applications automate data entry, improve accuracy, and enhance user interaction in various fields

## MNIST Handwritten Digits dataset

## 1. Data Exploration
- Load the MNIST Dataset: You can use the datasets.`MNIST class` from `torchvision` to load the dataset.
- Install required libraries
"""

import torch
import torchvision
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from torchvision import datasets, transforms,models
from sklearn.decomposition import PCA
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import random_split, DataLoader
from scipy.ndimage import gaussian_filter
import torch.nn.functional as F
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn import svm
from sklearn.metrics import confusion_matrix, classification_report
from torchvision.models import resnet18, ResNet18_Weights

"""### 1.1 MNIST Dataset Overview
- The MNIST (Modified National Institute of Standards and Technology) dataset is a benchmark for handwritten digit recognition, widely used in machine learning and computer vision.

**Key Characteristics**:
**1. Content**:

- Images: 70,000 grayscale images of handwritten digits (0-9).
    - Training Set: 60,000 images
    - Test Set: 10,000 images
      
**2. Image Details**:

- Dimensions: Each image is 28x28 pixels.
- Pixel Range: Values from 0 (black) to 255 (white).
- Normalization: Often normalized to a range of 0 to 1.
  
**3. Labels**:

- Each image is labeled with the corresponding digit (0-9).
  
**4. Variability**:

- Includes diverse handwriting styles from different contributors.
- Some images may contain noise or distortion.
  
**5. Accessibility**:

- Available in various formats via libraries like TensorFlow and PyTorch, or in CSV format.
  
**6Applications**:

- Used for training and testing classification models, including traditional algorithms and neural networks (e.g., CNNs).
"""

transform = transforms.Compose([
    transforms.ToTensor(),  # Convert images to PyTorch tensors
    transforms.Normalize((0.5,), (0.5,))  # Normalize pixel values to range [-1, 1]
])

"""### Dataset Description
The MNIST Handwritten Digits dataset is a classic benchmark in machine learning, widely used for image classification tasks.

It contains 70,000 grayscale images of handwritten digits from 0 to 9, split into 60,000 training and 10,000 test images.

Each image is a 28x28 pixel grid, flattened into a 784-dimensional vector, with labels indicating the digit each image represents.

### Classification Task
The objective is to classify each image into one of ten classes (digits 0-9) using a machine learning model.

This involves training a model to recognize patterns in the pixel data and accurately predict the digit in new images.
"""

# Load the dataset
train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)
test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)

# Create data loaders for batching
train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)
test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)

# Check dataset size
print(f'Training set size: {len(train_dataset)} images')
print(f'Test set size: {len(test_dataset)} images')

"""### 1.2 Visuaization of Random Images
**Handwriting Style and Data Variety in MNIST**
- **Diversity of Handwriting**:

    - Includes samples from various individuals, leading to significant variability in handwriting styles.
  
    - Captures individual differences such as stroke thickness, spacing, and curvature.

- **Digit Representation**:

    - All ten digits (0-9) are represented with numerous variations, enhancing model generalization.
      
- **Noise and Distortions**:

    - Images may contain noise and distortions due to different writing instruments, paper quality, and scanning conditions.
    
- **Different Writing Techniques**:

    - Captures both printed and cursive styles, with variations in slant and angle.
      
**Importance**

The diversity in handwriting styles and data variety makes MNIST a valuable benchmark for training and evaluating machine learning models, ensuring they can recognize handwritten digits in real-world conditions.
"""

# Function to display random images
def display_random_images(data_loader, num_images=5):
    data_iter = iter(data_loader)
    images, labels = next(data_iter)  # Use next() instead of .next()

    fig, axes = plt.subplots(1, num_images, figsize=(10, 2))
    for i in range(num_images):
        img = images[i].numpy().squeeze()  # Remove extra dimensions
        axes[i].imshow(img, cmap='gray')
        axes[i].set_title(f'Label: {labels[i].item()}')
        axes[i].axis('off')
    plt.show()

# Display 5 random images
display_random_images(train_loader)

# Explore a single sample
image, label = train_dataset[0]
print(f'Image shape: {image.shape}')  # Should be (1, 28, 28) for a single image
print(f'Label: {label}')  # Should be the digit label (0-9)

"""### 1.3 Inspect Dataset Structure, Size, and Labels

- Balanced Dataset: Each digit (0-9) has a similar number of samples, preventing bias toward any specific digit.

- Adequate Representation: Each class has between 5,421 and 6,742 samples, allowing the model to learn each digit well.

- Supports Generalization: The balance helps the model generalize effectively across all digit classes, improving accuracy on new data.
"""

from torch.utils.data import DataLoader

# Define a DataLoader for the training set
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)

# Get all the labels in the training set
train_labels = [label for _, label in train_loader.dataset]

# Check the balance of the labels
unique, counts = np.unique(train_labels, return_counts=True)
label_distribution = dict(zip(unique, counts))
label_distribution = {int(k): int(v) for k, v in label_distribution.items()}

print("Label distribution in training set:", label_distribution)

"""### 1.4 View Image Properties

- Dimensions: The images are 28x28 pixels with a single channel (torch.Size([1, 28, 28])), indicating they are grayscale images.

- Format: The dataset is indeed in grayscale, as expected.

- Pixel Value Range: The pixel values range from -1.0 to 1.0, which suggests that the images were normalized to this range (commonly used in machine learning preprocessing).

  
This means the dataset is correctly formatted for training, with consistent dimensions and grayscale format, and the pixel values are normalized for easier model learning.
"""

# Get one batch of images and labels
data_iter = iter(train_loader)
images, labels = next(data_iter)

# Confirm image dimensions and format
print("Image Properties:")
print(f"Batch shape: {images.shape}")
print(f"Single image shape: {images[0].shape}")
print(f"Image data type: {images.dtype}")
print(f"Min pixel value: {images.min().item()}")
print(f"Max pixel value: {images.max().item()}")

# Visualize the first image in the batch to confirm dimensions and grayscale format
plt.imshow(images[0].squeeze(), cmap='gray')
plt.title(f"Label: {labels[0].item()}")
plt.axis('off')
plt.show()

# Verify that all images in the batch are 28x28 grayscale
all_28x28 = all(image.shape == (1, 28, 28) for image in images)
if all_28x28:
    print("All images are confirmed to be 28x28 pixels in grayscale format.")
else:
    print("Warning: Not all images are 28x28 pixels in grayscale format.")

"""### 1.5 Classs Distribution Anaysis
We’ll plot the distribution of digit labels (0-9) to check balance.
"""

# Example label distribution (replace with actual distribution)
label_distribution = {0: 5923, 1: 6742, 2: 5958, 3: 6131, 4: 5842, 5: 5421, 6: 5918, 7: 6265, 8: 5851, 9: 5949}
labels = list(label_distribution.keys())
sizes = list(label_distribution.values())
train_labels = [label for label, count in label_distribution.items() for _ in range(count)]  # Sample dataset for histogram

# Set up a 2x2 grid of subplots
fig, axes = plt.subplots(2, 2, figsize=(15, 10))

# Pie Chart
axes[0, 0].pie(sizes, labels=labels, autopct='%1.1f%%', startangle=140, colors=plt.cm.Paired.colors)
axes[0, 0].set_title("Class Distribution (Pie Chart)")

# Bar Plot
axes[0, 1].bar(labels, sizes, color='skyblue')
axes[0, 1].set_xlabel("Digit Class")
axes[0, 1].set_ylabel("Frequency")
axes[0, 1].set_title("Class Distribution (Bar Plot)")

# Heatmap
sns.heatmap([sizes], annot=True, fmt="d", cmap="YlGnBu", cbar=False, ax=axes[1, 0], xticklabels=labels)
axes[1, 0].set_xlabel("Digit Class")
axes[1, 0].set_title("Class Distribution (Heatmap)")

# Histogram
sns.histplot(train_labels, bins=np.arange(11) - 0.5, kde=False, ax=axes[1, 1])
axes[1, 1].set_xlabel("Digit Label")
axes[1, 1].set_ylabel("Frequency")
axes[1, 1].set_title("Class Distribution (Histogram)")
axes[1, 1].set_xticks(range(10))

plt.tight_layout()
plt.show()

"""### 1.6 Image Intensity Analysis
- The negative mean suggests that the images might have been normalized to a range where dark pixel values are emphasized.
  
- The moderate standard deviation indicates reasonable contrast, which is beneficial for distinguishing between different handwritten digits.
"""

# Calculate the mean and standard deviation of pixel intensities
all_images = torch.cat([images.view(images.size(0), -1) for images, _ in train_loader], dim=0)
mean_intensity = all_images.mean().item()
std_intensity = all_images.std().item()

print(f'Mean pixel intensity: {mean_intensity:.4f}, Standard deviation: {std_intensity:.4f}')

"""### 1.7 Noise and Quality Check

There are a couple of unclear images and this might be due to:

- Improper Resizing/Cropping – Digits may be distorted if images aren't resized correctly.
  
- Aggressive Augmentation – Large rotations, scaling, or shifts could be making some digits look noisy.

- Normalization Errors – Using incorrect normalization values for grayscale images can introduce artifacts.

- Data Corruption – Some images may be corrupted, leading to noise.

"""

# Display random images to check noise and quality
display_random_images(train_loader)

"""### 1.8  Shape and Format Consistency Check

All images are valid and they meet the expected speciffications of `28*28`
"""

# Check for shape and format consistency
def check_image_shape_and_format(dataset):
    correct_shape = (1, 28, 28)  # Expected shape for MNIST images
    consistent_format = True
    inconsistent_shapes = []

    for i in range(len(dataset)):
        image, _ = dataset[i]
        if image.shape != correct_shape:
            inconsistent_shapes.append((i, image.shape))
            consistent_format = False

    if consistent_format:
        print("All images have the expected 28x28 shape and grayscale format.")
    else:
        print("Some images have inconsistent shapes:")
        for index, shape in inconsistent_shapes:
            print(f"Image {index} has shape {shape}")

# Run the check on the training dataset
check_image_shape_and_format(train_dataset)

"""## 2. Preprocessing Steps
Objective: Prepare the images for model training by ensuring uniformity and enhancing data quality.

### 2.1 Normalization using Z-Score
Normalization is an essential preprocessing step in machine learning that helps to standardize the range of independent variables or features of the data.


**1. Image Shape**:

- The image is correctly shaped as `torch.Size([1, 28, 28)`, indicating it has one channel (grayscale) and dimensions of 28x28 pixels, suitable for input into a convolutional neural network (CNN).
  
**2. Mean Pixel Value**:

- The mean pixel value after normalization is approximately `-0.4223`.
This indicates that, on average, the pixel intensities in the dataset are slightly below the overall mean of the dataset, which is expected after Z-score normalization but suggests a potential skew in the data.

**3. Standard Deviation of Pixel Values**:

- The standard deviation is approximately `0.00398`.

- This small value implies that the normalized pixel values have very little variation around the mean, indicating a tightly clustered distribution. Ideally, the standard deviation should be close to `1`.

**4.Considerations**

- The negative mean and low standard deviation may suggest the influence of outliers in the dataset, which can affect the normalization process.

- It may be beneficial to inspect the dataset for any outlier images or consider other normalization techniques if the results do not improve model performance.
"""

# Load the MNIST dataset
train_dataset = datasets.MNIST(root='./data', train=True, download=True)

# Convert to numpy array for mean and std calculations
train_images = np.array([np.array(train_dataset[i][0]) for i in range(len(train_dataset))])

# Calculate mean and standard deviation
mean = np.mean(train_images, axis=(0, 1, 2))  # Mean over the height and width of the images
std = np.std(train_images, axis=(0, 1, 2))    # Std over the height and width of the images

# Define the transformation for Z-score normalization
transform = transforms.Compose([
    transforms.ToTensor(),  # Convert to tensor (values in [0, 1])
    transforms.Normalize((mean,), (std,))  # Normalize using mean and std
])

# Load the MNIST training dataset with Z-score normalization
train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)

# Example: Retrieve an image and its label
image, label = train_dataset[0]  # Get the first image and its label

# Display the normalized image properties
print("Image shape:", image.shape)  # Should be (1, 28, 28)
print("Mean pixel value after normalization:", image.mean().item())  # Should be close to 0
print("Standard deviation of pixel values after normalization:", image.std().item())  # Should be close to 1

"""### 2.2 Reshaping

Reshaping involves changing the dimensions or structure of your data without altering the actual data values

**Purpose**:

- Compatibility: Many machine learning models, especially neural networks, require input data in a specific shape or format. For example, convolutional neural networks (CNNs) typically expect image data in the shape of `(N, C, H, W)`, where:
    - N: Number of images (batch size)
    - C: Number of channels (e.g., 1 for grayscale images, 3 for RGB images)
    - H: Height of the image (in pixels)
    - W: Width of the image (in pixels)
"""

# Function to reshape an image to a 1D array
def reshape_image(image):
    return image.view(-1)  # Converts 28x28 tensor to a 784-element 1D tensor

# Test the reshaping function on a single image from the dataset
# Grab one image and label from the train dataset
image, label = train_dataset[0]

# Print original and reshaped image shapes
print("Original image shape:", image.shape)
reshaped_image = reshape_image(image)
print("Reshaped image shape:", reshaped_image.shape)

"""### 2.3 Checking for Duplicate Samples
Check if there any duplicates in the dataset
"""

import torch
import numpy as np
from torchvision import datasets, transforms

# Set up a transform to convert the images to tensor and normalize
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,)),  # Normalizing to [-1, 1]
])

# Load the MNIST dataset
train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)

# Extract images and labels
train_images = train_dataset.data.numpy()  # Convert images to numpy array
train_labels = train_dataset.targets.numpy()  # Convert labels to numpy array

# Now, let's check for duplicates
# Flatten the images for comparison
flat_images = train_images.reshape(train_images.shape[0], -1)  # Shape: [N, 784]

# Use a set to track unique images
unique_images = set()
duplicates_count = 0

for img in flat_images:
    img_tuple = tuple(img)  # Convert the image array to a tuple for set storage
    if img_tuple in unique_images:
        duplicates_count += 1
    else:
        unique_images.add(img_tuple)

# Results
print(f"Number of duplicate images: {duplicates_count}")
print(f"Total unique images: {len(unique_images)}")
print(f"Total images in the dataset: {train_images.shape[0]}")

"""### 2.5  Train-test split
Perfom a further split from the training dataset to create a validation dataset. (80/20) rule
"""

# Define the split sizes for an 80-20 split on the training data
train_size = int(0.8 * len(train_dataset))  # 80% of the training data
val_size = len(train_dataset) - train_size  # Remaining 20% as validation data

# Split the training dataset into train and validation sets
train_data, val_data = random_split(train_dataset, [train_size, val_size])

# Data loaders for batching
train_loader = DataLoader(dataset=train_data, batch_size=64, shuffle=True)
val_loader = DataLoader(dataset=val_data, batch_size=64, shuffle=True)
test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)

# Verify the sizes of each set
print(f"Training set size: {len(train_data)}")
print(f"Validation set size: {len(val_data)}")
print(f"Test set size: {len(test_dataset)}")

"""### 2.6 Data Augmentation
Implemented the following Data Augmentation techniques:

- Rotation: Randomly rotate images by a specified degree (e.g., -10 to +10 degrees).
- Translation: Randomly shift images along the x and y axes.
- Zoom: Randomly zoom in or out of the image by a certain factor.
- Shearing: Apply a shearing transformation that distorts the image along an axis.
- Flipping: Randomly flip images horizontally or vertically.
- Cropping: Randomly crop images from the original size, potentially including random scaling.
- Bright Adjustment: Randomly change the brightness of the image, making it lighter or darker.
- Contrast Adjustment: Randomly adjust the contrast of the image to make features more or less prominent.
- Color jitter: Randomly change the saturation, hue, and brightness of the image to introduce color variations.
- Affine Transformations: Apply affine transformations, which include rotation, translation, and scaling in a single operation.
- Normalization: While not a transformation per se, normalizing pixel values can help in data augmentation to ensure consistency.
"""

# from torchvision import transforms

# data_transforms = transforms.Compose([
#     transforms.RandomRotation(degrees=10),     # Rotate randomly
#     transforms.RandomHorizontalFlip(),          # Flip horizontally
#     transforms.RandomVerticalFlip(),            # Flip vertically
#     transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),  # Random translation
#     transforms.ColorJitter(brightness=0.2, contrast=0.2),       # Change brightness/contrast
#     transforms.RandomResizedCrop(size=(28, 28), scale=(0.8, 1.0)),  # Random crop and resize
#     transforms.ToTensor(),                      # Convert image to tensor
#     transforms.Normalize((0.5,), (0.5,)),      # Normalize to [-1, 1]
# ])


# Original augmentation transform for training set
augment_transform = transforms.Compose([
    transforms.RandomRotation(degrees=15),  # Random rotation up to 15 degrees
    transforms.RandomResizedCrop(size=28, scale=(0.9, 1.1)),  # Random zoom in/out
    transforms.ToTensor()  # Convert to tensor
])

# Additional data augmentation transforms
data_transforms = transforms.Compose([
    transforms.RandomRotation(degrees=10),     # Rotate randomly
    transforms.RandomHorizontalFlip(),          # Flip horizontally
    transforms.RandomVerticalFlip(),            # Flip vertically
    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),  # Random translation
    transforms.ColorJitter(brightness=0.2, contrast=0.2),       # Change brightness/contrast
    transforms.RandomResizedCrop(size=(28, 28), scale=(0.8, 1.0)),  # Random crop and resize
    transforms.ToTensor(),                      # Convert image to tensor
    transforms.Normalize((0.5,), (0.5,)),      # Normalize to [-1, 1]
])

# Load the MNIST training set with new data augmentation
train_dataset_augmented = datasets.MNIST(root='./data', train=True, download=True, transform=data_transforms)

# Load the test dataset without augmentation
test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,)),      # Same normalization as in training
]))

# Data loaders for batching
train_loader_augmented = DataLoader(dataset=train_dataset_augmented, batch_size=64, shuffle=True)
test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)

# Visualize a few augmented images to verify transformations
def show_augmented_images(loader, num_images=5):
    plt.figure(figsize=(10, 2))
    for i, (image, label) in enumerate(loader):
        if i >= num_images:
            break
        plt.subplot(1, num_images, i + 1)
        plt.imshow(image[0].numpy().squeeze(), cmap='gray')  # Display the first image in the batch
        plt.title(label[0].item())
        plt.axis('off')
    plt.show()

# Display a few examples of augmented images
show_augmented_images(train_loader_augmented)

"""### 2.7 Dimensionality Reduction using PCA

**1. Purpose of PCA**:

- Dimensionality Reduction: Reduces the dataset's dimensionality from 784 to 50 features while retaining significant variance and information.
- Noise Reduction: Eliminates noise in the data, focusing on the most informative components.
- Computational Efficiency: Leads to faster training times for machine learning models due to fewer features.
- Improved Model Performance: Reduces overfitting by simplifying the dataset, which can enhance model generalization.
- Data Visualization: Enables visualization of high-dimensional data in lower dimensions for better understanding.
  
**2. Outcome of PCA**:

- Original Shape: The dataset consists of 60,000 samples, each represented by a 784-dimensional feature vector.
- Reduced Shape: After applying PCA, the dataset is transformed to 60,000 samples, each represented by a 50-dimensional feature vector.

**3. Implications**:

- Retained Variance: Important information is preserved in the reduced dimensions.
- Improved Efficiency: Training machine learning models on the reduced dataset is faster and less resource-intensive.
- Less Overfitting: The reduced complexity helps prevent overfitting, leading to better performance on unseen data.
"""

# Convert the dataset to flattened arrays for PCA (28x28 -> 784)
def flatten_data(dataset):
    data = []
    labels = []
    for img, label in dataset:
        img_flat = img.view(-1).numpy()  # Flatten the 28x28 image to a 784-element array
        data.append(img_flat)
        labels.append(label)
    return np.array(data), np.array(labels)

train_data, train_labels = flatten_data(train_dataset)
test_data, test_labels = flatten_data(test_dataset)

# Apply PCA to reduce dimensionality, preserving 95% of variance
pca = PCA(n_components=0.95, random_state=42)
train_data_pca = pca.fit_transform(train_data)
test_data_pca = pca.transform(test_data)

# Print the reduced shape and plot explained variance
print("Original shape:", train_data.shape)
print("Reduced shape:", train_data_pca.shape)

plt.figure(figsize=(8, 4))
plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.xlabel('Number of Components')
plt.ylabel('Explained Variance Ratio')
plt.title('Explained Variance by PCA Components')
plt.show()

# Display a PCA-transformed image by reconstructing
def display_reconstructed_image(data_pca, index):
    img_pca = pca.inverse_transform(data_pca[index])
    img = img_pca.reshape(28, 28)
    plt.imshow(img, cmap='gray')
    plt.title(f"Label: {train_labels[index]}")
    plt.axis('off')
    plt.show()

# Display one reconstructed image from PCA-transformed data
display_reconstructed_image(train_data_pca, 0)

"""### 2.8 Binarization
Binarization is a process of converting grayscale images into binary images, where each pixel is either black (0) or white (1). This transformation simplifies the data and emphasizes the key features (in this case, the shapes of the digits) by removing unnecessary details.

"""

# Function to binarize an image
def binarize_image(image_tensor, threshold=0.5):
    """Convert a grayscale image tensor to a binary image using a threshold."""
    # Apply thresholding
    binary_image = (image_tensor > threshold).float()
    return binary_image

# Function to binarize an entire dataset
def binarize_dataset(dataset, threshold=0.5):
    binarized_data = []
    labels = []
    for img, label in dataset:
        img_binarized = binarize_image(img)
        binarized_data.append(img_binarized.numpy().squeeze())  # Convert tensor to numpy and remove channel dim
        labels.append(label)
    return binarized_data, labels

# Binarize the training dataset
train_data_binarized, train_labels = binarize_dataset(train_dataset)

# Function to display original and binarized images
def show_original_and_binarized_images(dataset, binarized_images, num_images=5):
    plt.figure(figsize=(10, 5))
    for i in range(num_images):
        # Original Image
        plt.subplot(2, num_images, i + 1)
        plt.imshow(dataset[i][0].numpy().squeeze(), cmap='gray')
        plt.title(f'Original {i+1}')
        plt.axis('off')

        # Binarized Image
        plt.subplot(2, num_images, i + 1 + num_images)
        plt.imshow(binarized_images[i], cmap='gray')
        plt.title(f'Binarized {i+1}')
        plt.axis('off')

    plt.tight_layout()
    plt.show()

# Display a few original and binarized images
show_original_and_binarized_images(train_dataset, train_data_binarized, num_images=5)

"""### 2.8.1 Noise Reduction
- Noise Reduction helps to improve image quality by removing unwanted variations and disturbances.
- Gaussian Filter: A Gaussian filter smooths the image by averaging the pixels around a target pixel, weighted by their distance from the target. This is effective for removing Gaussian noise.

- Median Filter: A median filter replaces each pixel's value with the median of the pixel values in its neighborhood. This is particularly effective at removing salt-and-pepper noise.
"""

# Function to apply Gaussian filter for noise reduction
def apply_gaussian_filter(image, sigma=1):
    """Apply Gaussian filter to the image to reduce noise."""
    # Convert the tensor to a numpy array and apply the filter
    image_np = image.numpy()
    filtered_image = gaussian_filter(image_np, sigma=sigma)
    return filtered_image

# Function to apply noise reduction on the dataset
def reduce_noise_dataset(dataset):
    noise_reduced_data = []
    labels = []
    for img, label in dataset:
        img_filtered = apply_gaussian_filter(img.squeeze(), sigma=1)  # Squeeze to remove channel dimension
        noise_reduced_data.append(img_filtered)
        labels.append(label)
    return noise_reduced_data, labels

# Reduce noise in the training and test datasets
train_data_filtered, train_labels = reduce_noise_dataset(train_dataset)
test_data_filtered, test_labels = reduce_noise_dataset(test_dataset)

# Function to display original and filtered images side by side
def show_images_comparison(original_images, filtered_images, labels, num_images=5):
    plt.figure(figsize=(12, 4))
    for i in range(min(num_images, len(original_images))):
        # Original Image
        plt.subplot(2, num_images, i + 1)
        plt.imshow(original_images[i], cmap='gray')
        plt.title(f'Original\nLabel: {labels[i]}')
        plt.axis('off')

        # Filtered Image
        plt.subplot(2, num_images, i + 1 + num_images)
        plt.imshow(filtered_images[i], cmap='gray')
        plt.title('Filtered')
        plt.axis('off')

    plt.show()

# Display a few original and filtered images
original_images = [img.numpy().squeeze() for img, _ in train_dataset]
show_images_comparison(original_images, train_data_filtered, train_labels, num_images=5)

"""### 2.8.2 Standardization

Standardization is a preprocessing technique used in machine learning and statistics to center the data around a mean of 0 and scale it to have a standard deviation of 1. This is particularly useful for image data, as it helps in improving the convergence speed of certain algorithms and enhances the model's performance.

**1. Mean Pixel Value**: -0.7387

- **Interpretation**: The mean is extremely close to zero, which is the desired outcome of standardization. The small value (approximately zero) indicates that the pixel values are centered around zero.
- **Implication**: A mean of zero ensures that the data distribution is balanced, which helps in training machine learning models, especially those that rely on gradient descent optimization methods. This balance allows the models to learn more effectively without bias towards any particular direction.
  
**2. Standard Deviation**: 0.6162
- **Interpretation**: The standard deviation is very close to 1. This is also the expected outcome of standardization, which scales the data to have unit variance.
- **Implication**: A standard deviation of 1 ensures that the pixel values are on a consistent scale, which helps in reducing the sensitivity of the model to different ranges of input features. This uniformity allows the model to learn the underlying patterns in the data more effectively.

"""

# Function to calculate mean and standard deviation of the training dataset
def calculate_mean_std(dataset):
    # Concatenate all images into a single tensor
    all_images = torch.stack([img for img, _ in dataset])
    mean = all_images.mean()
    std = all_images.std()
    return mean, std

# Calculate mean and standard deviation for the training dataset
mean, std = calculate_mean_std(train_dataset)
print(f"Mean: {mean:.4f}, Standard Deviation: {std:.4f}")

# Function to standardize the dataset
def standardize_dataset(dataset, mean, std):
    standardized_data = []
    labels = []
    for img, label in dataset:
        # Standardize the image
        img_standardized = (img - mean) / std
        standardized_data.append(img_standardized)
        labels.append(label)
    return standardized_data, labels

# Standardize the training and test datasets
train_data_standardized, train_labels = standardize_dataset(train_dataset, mean, std)
test_data_standardized, test_labels = standardize_dataset(test_dataset, mean, std)

# Function to display a few standardized images
def show_standardized_images(standardized_images, labels, num_images=5):
    plt.figure(figsize=(12, 4))
    for i in range(min(num_images, len(standardized_images))):
        plt.subplot(1, num_images, i + 1)
        plt.imshow(standardized_images[i].numpy().squeeze(), cmap='gray')
        plt.title(f'Label: {labels[i]}')
        plt.axis('off')
    plt.show()

# Display a few standardized images
show_standardized_images(train_data_standardized, train_labels, num_images=5)

"""### 2.9 Dimensional Consistency Check
All images have the expected shape
"""

# Ensure all images are 28x28
for images, _ in train_loader:
    assert images.shape[2:] == (28, 28), "Inconsistent dimensions!"
print("All images have the expected 28x28 shape.")

"""## 3. Model Selection and Training

#### 3.1 Models used
"""

# Prepare data
X = []
y = []

# Convert images and labels to numpy arrays
for image, label in train_dataset:
    X.append(image.view(-1).numpy())  # Flatten the image
    y.append(label)

# Convert lists to numpy arrays
X = np.array(X)
y = np.array(y)

# Split the dataset into training and testing sets (80-20 ratio)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""#### 3.1.1  Logistic Regression"""

# Initialize and train the Logistic Regression model
logistic_model = LogisticRegression(max_iter=1000)
logistic_model.fit(X_train, y_train)

# Predict on the test set
y_pred = logistic_model.predict(X_test)

# Evaluate the model accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy of Logistic Regression on MNIST: {accuracy:.4f}')

# Generate the confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)

# Plot the confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=range(10), yticklabels=range(10))
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix of Logistic Regression on MNIST')
plt.show()

"""#### 3.1.2 K-Nearest Neighbors (KNN)"""

# Initialize and train the KNN model
knn_model = KNeighborsClassifier(n_neighbors=5)  # You can adjust n_neighbors as needed
knn_model.fit(X_train, y_train)

# Predict on the test set
y_pred = knn_model.predict(X_test)

# Evaluate the model accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy of K-Nearest Neighbors on MNIST: {accuracy:.4f}')

# Generate the confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)

# Plot the confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=range(10), yticklabels=range(10))
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix of K-Nearest Neighbors on MNIST')
plt.show()

"""#### 3.1.3 Support Vector Machine (SVM)"""

# Initialize and train the SVM model
svm_model = svm.SVC(kernel='rbf', gamma='scale')
svm_model.fit(X_train, y_train)

# Predict on the test set
y_pred = svm_model.predict(X_test)

# Evaluate the model accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy of Support Vector Machine on MNIST: {accuracy:.4f}')

# Generate the confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)

# Plot the confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=range(10), yticklabels=range(10))
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix of SVM on MNIST')
plt.show()

"""### 3.2 Deep Learning Models(MLP)
A **Multilayer Perceptron (MLP)** is a type of artificial neural network composed of multiple layers of interconnected neurons. It consists of:

- Input Layer: Receives input data.
- Hidden Layers: One or more layers that process the input using weights, biases, and activation functions (like ReLU, sigmoid, or tanh).
- Output Layer: Produces the final output (e.g., class probabilities).

#### 3.2.1 Define the MLP model
"""

class SimpleMLP(nn.Module):
    def __init__(self):
        super(SimpleMLP, self).__init__()
        self.fc1 = nn.Linear(28 * 28, 128)  # Input layer
        self.fc2 = nn.Linear(128, 64)        # Hidden layer
        self.fc3 = nn.Linear(64, 10)         # Output layer (10 classes for digits 0-9)
        self.relu = nn.ReLU()                 # Activation function

    def forward(self, x):
        x = x.view(-1, 28 * 28)  # Flatten the input
        x = self.relu(self.fc1(x))  # First layer with ReLU activation
        x = self.relu(self.fc2(x))  # Second layer with ReLU activation
        x = self.fc3(x)             # Output layer (no activation)
        return x

"""#### 3.2.2 Set up the Training Process"""

# Set device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

mlp_model = SimpleMLP().to(device)
criterion = nn.CrossEntropyLoss()  # Loss function for multi-class classification
optimizer = optim.Adam(mlp_model.parameters(), lr=0.001)  # Adam optimizer

"""#### 3.2.3 Train the MLP Model

**Key Observations**:
- TThe model's loss decreases over the epochs, starting from 0.1440 in Epoch 1 and reaching as low as 0.0218 by Epoch 8, indicating an overall learning trend.
- Some fluctuations in loss (e.g., an increase to 0.2837 in Epoch 4 and 0.1794 in Epoch 9) suggest minor instability, potentially from overfitting or optimization challenges.
- Despite these fluctuations, the model generally improves over time, achieving its best performance near Epoch 8, with losses in subsequent epochs remaining relatively low. This trend indicates the model’s overall ability to generalize and make better predictions as training progresses.
"""

num_epochs = 5  # You can increase this for better performance
for epoch in range(num_epochs):
    mlp_model.train()  # Set the model to training mode
    running_loss = 0.0
    for images, labels in train_loader:
        images, labels = images.to(device), labels.to(device)

        optimizer.zero_grad()  # Clear gradients
        outputs = mlp_model(images)  # Forward pass
        loss = criterion(outputs, labels)  # Compute loss
        loss.backward()  # Backward pass
        optimizer.step()  # Update weights

        running_loss += loss.item()  # Accumulate loss

    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}')

"""#### 3.2.4 Evaluate the MLP model

**Key Points**:
**1. High Accuracy**:

- An accuracy of 88.90% is considered quite impressive for image classification tasks, especially for the MNIST dataset, which consists of handwritten digits. This indicates that the model is proficient at recognizing and classifying the digit images.
    
**2. Model Perfomance**   

- This high accuracy suggests that the model has effectively learned the underlying patterns in the training data and is able to apply this knowledge to make accurate predictions on unseen data (the test set).

**3. Evaluation of Generalization**:

- A test accuracy of 88.90% implies good generalization capabilities, meaning the model is not merely memorizing the training data but is also able to perform well on new, unseen examples. This is a positive indication of the model’s robustness and reliability in practical applications.
"""

mlp_model.eval()  # Set the model to evaluation mode
correct = 0
total = 0
with torch.no_grad():
    for images, labels in test_loader:
        images, labels = images.to(device), labels.to(device)
        outputs = mlp_model(images)  # Forward pass
        _, predicted = torch.max(outputs.data, 1)  # Get the predicted labels
        total += labels.size(0)  # Total samples
        correct += (predicted == labels).sum().item()  # Correct predictions

accuracy = 100 * correct / total
print(f'Accuracy of the Simple MLP on the MNIST test set: {accuracy:.2f}%')

"""### 3.3 Convolutional Neural Network (CNN)
A Convolutional Neural Network (CNN) is a type of artificial intelligence model that's particularly good at understanding images. Think of it like a smart eye that can identify patterns and features in pictures, just as humans recognize faces or objects.

- Loss Improvement: The loss decreases from 0.1500 in the first epoch to 0.0082 by the tenth epoch, indicating that the model is effectively learning from the data.

- High Test Accuracy: The model achieves a test accuracy of 99.05%, meaning it correctly classifies about 99.23% of the test images.

**Summary**:
Overall, the CNN is performing very well, demonstrating both effective learning during training and strong generalization to new data.
"""

class SimpleCNN(nn.Module):
    def __init__(self):
        super(SimpleCNN, self).__init__()
        # Convolution Layer 1
        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1)  # 28x28x1 -> 28x28x32
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)  # Pooling Layer 1
        # Convolution Layer 2
        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)  # 28x28x32 -> 28x28x64
        # Fully Connected Layer
        self.fc1 = nn.Linear(64 * 7 * 7, 128)  # 7x7x64 = 3136
        self.fc2 = nn.Linear(128, 10)  # Output layer (10 classes)

    def forward(self, x):
        x = self.conv1(x)  # First convolution layer
        x = nn.ReLU()(x)  # Activation function
        x = self.pool(x)  # First pooling layer

        x = self.conv2(x)  # Second convolution layer
        x = nn.ReLU()(x)  # Activation function
        x = self.pool(x)  # Second pooling layer

        x = x.view(-1, 64 * 7 * 7)  # Flatten the tensor
        x = self.fc1(x)  # First fully connected layer
        x = nn.ReLU()(x)  # Activation function
        x = self.fc2(x)  # Output layer
        return x

"""#### 3.3.2 Instantiate the model, define loss function and optimizer"""

cnn_model = SimpleCNN().to(device)
criterion = nn.CrossEntropyLoss()  # Loss function for multi-class classification
optimizer = optim.Adam(cnn_model.parameters(), lr=0.001)  # Adam optimizer

"""#### 3.3.3 Training the model"""

num_epochs = 5  # You can increase this for better performance
for epoch in range(num_epochs):
    cnn_model.train()  # Set the model to training mode
    running_loss = 0.0
    for images, labels in train_loader:
        images, labels = images.to(device), labels.to(device)

        optimizer.zero_grad()  # Clear gradients
        outputs = cnn_model(images)  # Forward pass
        loss = criterion(outputs, labels)  # Compute loss
        loss.backward()  # Backward pass
        optimizer.step()  # Update weights

        running_loss += loss.item()  # Accumulate loss

    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}')

"""#### 3.3.4 Evaluating the model"""

cnn_model.eval()  # Set the model to evaluation mode
correct = 0
total = 0
with torch.no_grad():
    for images, labels in test_loader:
        images, labels = images.to(device), labels.to(device)
        outputs = cnn_model(images)  # Forward pass
        _, predicted = torch.max(outputs.data, 1)  # Get the predicted labels
        total += labels.size(0)  # Total samples
        correct += (predicted == labels).sum().item()  # Correct predictions

accuracy = 100 * correct / total
print(f'Accuracy of the Simple CNN on the MNIST test set: {accuracy:.2f}%')

"""### 3.4 Pre- trained models

#### 3.4.1 Load the pre-trained ResNet model
"""

# Load a pre-trained ResNet model with the updated weights parameter
pretrained_model = resnet18(weights=ResNet18_Weights.DEFAULT)
# Modify the first convolutional layer to accept 1-channel input
pretrained_model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
# Modify the model to match the number of output classes
num_ftrs = pretrained_model.fc.in_features  # Get the input features of the final layer
pretrained_model.fc = nn.Linear(num_ftrs, 10)  # Replace the final layer with 10 classes

# Move the model to the appropriate device
pretrained_model = pretrained_model.to(device)

# Define loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(pretrained_model.parameters(), lr=0.001)

"""#### 3.4.2 Fine-tune the model"""

num_epochs = 5  # You can increase this for better performance
for epoch in range(num_epochs):
    pretrained_model.train()  # Set the model to training mode
    running_loss = 0.0
    for images, labels in train_loader:
        images, labels = images.to(device), labels.to(device)

        optimizer.zero_grad()  # Clear gradients
        outputs = pretrained_model(images)  # Forward pass
        loss = criterion(outputs, labels)  # Compute loss
        loss.backward()  # Backward pass
        optimizer.step()  # Update weights

        running_loss += loss.item()  # Accumulate loss

    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}')

"""#### 3.4.3 Evaluating the model"""

pretrained_model.eval()  # Set the model to evaluation mode
correct = 0
total = 0
with torch.no_grad():
    for images, labels in test_loader:
        images, labels = images.to(device), labels.to(device)
        outputs = pretrained_model(images)  # Forward pass
        _, predicted = torch.max(outputs.data, 1)  # Get the predicted labels
        total += labels.size(0)  # Total samples
        correct += (predicted == labels).sum().item()  # Correct predictions

accuracy = 100 * correct / total
print(f'Accuracy of the Pre-trained ResNet on the MNIST test set: {accuracy:.2f}%')

"""## 4. Prediction and Evaluation
Objective: Test the model on unseen data and measure its performance.

#### 4.1: Make Predictions
"""

# Define a function for evaluation
def evaluate_model(model, test_loader, device):
    model.eval()  # Set the model to evaluation mode
    y_true = []
    y_pred = []

    with torch.no_grad():
        for images, labels in test_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)  # Forward pass
            _, predicted = torch.max(outputs.data, 1)  # Get the predicted labels

            y_true.extend(labels.cpu().numpy())  # Append true labels
            y_pred.extend(predicted.cpu().numpy())  # Append predicted labels

"""#### 4.2: Calculate Accuracy"""

accuracy = np.mean(np.array(y_true) == np.array(y_pred)) * 100
print(f'Accuracy of the model on the MNIST test set: {accuracy:.2f}%')

"""#### 4.3: Confusion Matrix"""

cm = confusion_matrix(y_true, y_pred)

# Visualizing the confusion matrix
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=range(10), yticklabels=range(10))
plt.xlabel('Predicted labels')
plt.ylabel('True labels')
plt.title('Confusion Matrix')
plt.show()

"""#### 4.3: Precision and Recall"""

report = classification_report(y_true, y_pred, target_names=[str(i) for i in range(10)])
print("Classification Report:")
print(report)

"""## 5. Saving the models"""

import joblib

# Assuming `logistic_model`, `knn_model`, and `svm_model` are your trained models
joblib.dump(logistic_model, 'logistic_model.pkl')
joblib.dump(knn_model, 'knn_model.pkl')
joblib.dump(svm_model, 'svm_model.pkl')

### 5.2. Saving the Deep Learning Models

# Assuming `mlp_model` and `cnn_model` are your trained models
torch.save(mlp_model.state_dict(), 'mlp_model.pth')
torch.save(cnn_model.state_dict(), 'cnn_model.pth')

# If using pre-trained models (like ResNet)
torch.save(pretrained_model.state_dict(), 'pretrained_model.pth')

"""### Challenges Faced During Exploration, Preprocessing, and Modeling"""

print("\nChallenges Faced:")
print("- Data Augmentation: Balancing the amount and type of transformations to prevent overfitting.")
print("- Noise Reduction: Finding the right filtering technique to improve image quality without losing important details.")
print("- Model Selection: Choosing between traditional and deep learning models based on data size and computational resources.")
print("- Hyperparameter Tuning: Optimizing model parameters for better performance.")

"""### Potential Improvements or Alternative Approaches"""

print("\nPotential Improvements:")
print("- Experiment with more advanced data augmentation techniques to improve model generalization.")
print("- Fine-tune hyperparameters using techniques like Grid Search or Random Search.")
print("- Explore other deep learning architectures like Transfer Learning with pre-trained models (e.g., ResNet, VGG).")
print("- Consider ensemble methods to combine the predictions from multiple models for improved accuracy.")

